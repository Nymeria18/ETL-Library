from pyspark.sql import SQLContext
import pyspark.sql.functions
from pyspark.sql.functions import avg,col
from pyspark.mllib.util import MLUtils
from pyspark.ml.feature import Normalizer

sqlContext=SQLContext(sc)
form=raw_input("Select the input format\n1.csv\n2.json\n3.jbdc\n4.orc\n5.paraquet\n6.libsvm\n7.text")
form=str(form)
path=raw_input("Enter the path")
path=str(path)
df=sqlContext.read.format(form).option("header","True").option("inferSchema", "True").load(path)

df.dtypes
print("No. of rows in the dataframe="+str(df.count()))
print("No. of columns in the dataframe="+str(len(df.columns)))

Nulls={}
for x in range(len(df.columns)):
            dfNoNulls=df.na.drop(how="all",subset=df.columns[x])
            Nulls[df.columns[x]]=((df.count()-dfNoNulls.count()))
print Nulls
threshd=input("Threshold value of missing rows to drop column?")
for x in Nulls:
    if Nulls[x]>=threshd:
        print("dropping "+x)
        df=df.drop(x)

print("Imputing Mean value in the rest of the missing value numeric columns")
nonnull=df.na.drop(how="all")
for x in Nulls:
        if Nulls[x]>0 and Nulls[x]<=threshd:
           meanValue=nonnull.agg(avg(col(x))).first()[0]
           print(x, meanValue)
           if type(meanValue) is float:
               df=df.na.fill(meanValue, [x])

Nulls={}
for x in range(len(df.columns)):
        dfNoNulls=df.na.drop(how="all",subset=df.columns[x])
        Nulls[df.columns[x]]=((df.count()-dfNoNulls.count()))

print("Dropping missing value rows of non numeric columns")
for x in Nulls:
        if Nulls[x]>0:
             df=df.na.drop(subset=[x])

intp=[]
for x,y in df.dtypes:
        if y=='int':
             intp.append(x)

Normopt=raw_input("Performing nomalization\nSelect type:\n1.$L^1$ norm\n2.$L^\infty$")
if Normopt is "$L^1$":
        for x in intp:
             normalizer = Normalizer(inputCol=x, outputCol=x, p=1.0)
             l1NormData = normalizer.transform(df)
elif Normopt is "$L^\infty$":
        for x in intp:
             lInfNormData = normalizer.transform(df, {normalizer.p: float(x)})

a=intp
b=intp

li=[]

for x in a:
        for y in b:
             if x is not y:
                    li.append(x)
                    li.append(y)
                    li.append(df.corr(x,y))

pair=[]

for x in range(0,len(li)/3):
        temp=[]
             temp.append(li[x*3])
             temp.append(li[(x*3)+1])
             temp.append(li[(x*3)+2])
             pair.append(temp)

threscorr=input("Enter the correlation threshold")
print("The feature pairs with corr coeff more than threshold are:")

for x in range(0,len(pair)):
        corr=pair[x][2]
             if corr>=threscorr:
                   print[pair[x]]
                   df=df.drop(pair[x][0])
                   df=df.drop(pair[x][1])

print("Exporting dataframe to",form)
pathsave=raw_input("The path to save to")
df.write.format(form).save(pathsave)
